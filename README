ceph-ansible-playbook - V 2.0
=============================

Contact Information :
For any suggestion or BUG report , please write to karan.singh@csc.fi


Supported Ceph Components

* Monitors
* OSDs
* MDS
* Ceph admin node
* RADOSGW

Changes to this version of ceph-ansible-playbook:
* ceph.repo has added with priority=5 , so to override ceph packages from EPEL
* Added yum-plugin-priorities package as a dependency package
* Installs xfsprogs package for XFS filesystem 
* Rados gateway configuration



This ansible script can be used to :

* Hands free ceph cluster Deployment which includes MON , OSD and MDS. You can use "create_cluster.yml" ansible playbook for this purpose.

* ADD OSD nodes to your exixting cluster ( scaling up your cluster ) . For this purpose please use "create_osd.yml" ansible playbook.

* ADD MONITOR nodes to your existing cluster ( in a 3 MON node cluster , this will add 2 more MON nodes making it total 5 MON node cluster ). This is not usually required , please make use of "create_mon.yml" , if you are sure for using 5 monitor nodes.

* ADD MDS node to your existing cluster . For this purpose please use "create_mds.yml" ansible playbook.

* ADD ceph client keyrings  to a node that you want to use as ceph admin node to monitor and mange your cluster. For this prupose please use "create_ceph-admin.yml" ansible playbook.




Details:

* Monitors deployment - Minimum 3 monitor nodes are required for mointor deployment using this ansible playbook.

* Object Storage Daemons -  You can start with a minimum 1 node and then grow this number. The playbook either supports journal and data to be on same disk.

* Metadata daemons - You can start with 1 MDS server , you can grow MDS serer number but its not officially supported in production.

* Config files Collocation -  The playbook supports collocating Monitors, OSDs and MDSs configuration files on the same machine used for ansible playbook execution.

* The playbook was has been  validated on CentOS 6.4 , it should ideally work on other RHEL based variants.

* Tested on Ceph Dumpling , Emperor and Development 0.78 release.




## How to use 

1. Install Ansible rpm on one of your node  # yum install -y ansible  
2. Make sure you are able to do passwordless ssh from Ansible node to other nodes that you are planning for ceph cluster.
3. Untar this playbook to any directory , ( i would prefer under /etc/ansible )
4. Copy ansible host file : cp /etc/ansible/ceph-playbook/hosts /etc/ansible/hosts 
5. Edit /etc/ansible/hosts  with hostnames of your MON , OSD , MDS and Ceph-Admin hostnames
6. Test if ansible can access the nodes # ansible all -m ping

# ansible all -m ping
storage0103-ib | success >> {
    "changed": false,
    "ping": "pong"
}
```

7. Ready to deploy? Let's go!

# ansible-playbook -f 7 -v create_cluster_with_1_monitor.yml 

8. Monitor ansible output and once successfull , check your ceph cluster status.



Future features:

* Rolling upgrade -  Upgrade from  Emperor to 0.78 to 0.80 ( firefly )


What Your Should Change before running this playbook

* Check ceph version that has to be installed under groupvars/all
* Check you have appropriate proxy set under roles/common/tasks/main.yml and roles/common/templates/macros
